"""Provides for generating and batching data, and converting it into the tf.data.Dataset format expected by TensorFlow.
"""

import multiprocessing as mp
import numpy as np
import os
import tensorflow as tf
import tools

tfd = tf.data
# noinspection PyCallingNonCallable
mp = mp.get_context(method='fork')


class BatchData:
    """Feeds (feature, label) pairs generated by a pure-Python function into a tf.data.Dataset. Primarily useful for
    expensive Python functions, as it utilises multiprocessing to call the function multiple times in parallel. Note
    that its terminate() method should be called when the instance is finished with.

    The Python function in question must therefore be suitable for use with multiprocessing. If it is not, then the
    threading can be disabled. The option is only really provided for a consistent interface, as BatchData doesn't do
    very much then.
    """

    def __init__(self, data_fn=None, batch_size=1, queue_size=50, dtype=None, shape=None, num_processes=None,
                 use_processes=True):
        """Initialising this class will create a queue of length :queue_size: and start populating it with the return
        values from :data_fn:. The argument :num_processes: determines how many processes will be used to call
        :data_fn:. (Note then that calls of :data_fn: might return their results out of order with the order that they
        were called in.) The default number of processes is os.cpu_count().
        
        
        The argument :batch_size: is used to determine the size of the batches that it later produces. The arguments
        :dtype:, :shape: should be the dtype and shape of the result of :data_fn:, as a 'nested structure', that is a
        structure which matches that of the result of :data_fn:. If either of them are set to None (the default), then
        :data_fn: will be called once to attempt to determine these values automatically, but so far this automatic
        inference only supports the simple cases of the result being either a numpy array or a tuple of numpy arrays.
        """
        if dtype is None or shape is None:
            val = data_fn()
            try:
                dtype = val.dtype
                shape = val.shape
            except AttributeError:
                try:
                    dtype = tuple(v.dtype for v in val)
                    shape = tuple(v.shape for v in val)
                except AttributeError as e:
                    raise CannotInferDtypeShape(e) from e

            def data_generator():
                yield val
                while True:
                    yield data_fn()

        else:
            def data_generator():
                while True:
                    yield data_fn()

        self.dtype = dtype
        self.shape = shape

        self.batch_size = batch_size
        self.terminated = False

        if use_processes:
            self.queue = mp.Queue(maxsize=queue_size)

            def _gen_one_data(thread, max_thread):
                def gen_one_data_wrapper():
                    if hasattr(data_fn, 'thread_prepare'):
                        data_fn.thread_prepare(thread, max_thread)
                    data_gen = data_generator()
                    while True:
                        self.queue.put(next(data_gen))
                return gen_one_data_wrapper

            if num_processes is None:
                num_processes = os.cpu_count()
            self.processes = [mp.Process(target=_gen_one_data(i, num_processes))
                              for i in range(num_processes)]

            for process in self.processes:
                process.start()

            def generator():
                while True:
                    yield self.queue.get()
            self.generator = generator
        else:
            self.processes = []

            def generator():
                if hasattr(data_fn, 'thread_prepare'):
                    data_fn.thread_prepare(1, 1)
                yield from data_generator()
            self.generator = generator
        
    def __call__(self):
        """Creates a tf.data.Dataset that gives batches of the appropriate size."""
        
        if not self.terminated:
            # As we want a Dataset that keeps producing (feature, label) pairs potentially forever, we have to use the
            # from_generator constructor. (I don't think any of the others allow for online data production like this.)
            ds = tfd.Dataset.from_generator(self.generator, self.dtype, self.shape)
            return ds.batch(self.batch_size)
        else:
            raise TerminatedBatchData

    def __del__(self):
        self.terminate()
    
    def terminate(self):
        """Terminates the processes that this instance uses."""
        for process in self.processes:
            process.terminate()
            process.join()
        self.terminated = True
            
    @classmethod
    def context(cls, *args, **kwargs):
        """For use in with statements. Creates a BatchData and automatically terminates it afterwards."""

        # noinspection PyMethodParameters
        class _BatchDataContext:
            def __enter__(self_context):
                self = cls(*args, **kwargs)
                self_context.instance = self
                return self
            
            def __exit__(self_context, exc_type, exc_val, exc_tb):
                self_context.instance.terminate()
                
        return _BatchDataContext()
    
    @classmethod
    def batch(cls, data_fn, batch_size=1):
        """Takes a function :data_fn: which returns a generator and a :batch_size:, which defaults to 1, and returns a
        batch of that size. Its return value is not wrapped in a tf.data.Dataset.
        """
        
        with cls.context(data_fn=data_fn) as self:
            val = self.queue.get()
            batches = tuple(np.array([v for _ in range(batch_size)]) for v in val)
            for i in range(1, batch_size):
                val = self.queue.get()
                for j, v in enumerate(val):
                    batches[j][i] = v
            return batches
        
    @staticmethod
    def to_dataset(data):
        """Returns a tf.data.Dataset which endlessly repeats :data:."""
        # Lambda wrapper is because it has to be called later on to be part of the same graph as the Estimator.
        return lambda: tfd.Dataset.from_tensors(data).repeat()


class TerminatedBatchData(tools.DefaultException, RuntimeError):
    """Raised when an instance of BatchData has been terminated and is then called again."""
    default_msg = 'BatchData has already been terminated.'


class CannotInferDtypeShape(RuntimeError):
    """Raised when BatchData is unable to infer either the dtype or the shape of the data."""
